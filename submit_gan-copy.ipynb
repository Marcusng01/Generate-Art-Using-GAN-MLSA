{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Azure ML GAN Expriment\n",
        "\n",
        "To use this notebook, you need to download `config.json` file from Azure ML Workspace and place it in this folder. This will allow us to get the workspace reference right away:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1694326676427
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "dataset = r'C:\\Marcus\\Important Docs\\MLSA\\WikiArt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1694359933300
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing interactive authentication. Please follow the instructions on the terminal.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactive authentication successfully completed.\n",
            "GAN-art-generator\tsoutheastasia\tAzure-ML-Workshop\tsoutheastasia\n",
            "Library configuration succeeded\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "try:\n",
        "    ws = Workspace.from_config()\n",
        "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
        "    print('Library configuration succeeded')\n",
        "except:\n",
        "    print('Workspace not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then make sure we have the compute cluster. If the cluster does not exist - we will create it programmatically!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1694359940836
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing cluster, use it.\n",
            "\n",
            "Running\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster\n",
        "cluster_name = \"GAN\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
        "                                                           vm_priority='lowpriority',\n",
        "                                                           min_nodes=1,\n",
        "                                                           max_nodes=4)\n",
        "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "cluster.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now upload the images dataset into the Azure ML Workspace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1694359942971
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating arguments.\n",
            "Arguments validated.\n",
            "'overwrite' is set to True. Any file already present in the target will be overwritten.\n",
            "Uploading files from 'C:/Marcus/Important Docs/MLSA/WikiArt' to 'gan_data'\n",
            "Creating new dataset\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{\n",
              "  \"source\": [\n",
              "    \"('workspaceblobstore', '/gan_data')\"\n",
              "  ],\n",
              "  \"definition\": [\n",
              "    \"GetDatastoreFiles\"\n",
              "  ]\n",
              "}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "ds = ws.get_default_datastore()\n",
        "Dataset.File.upload_directory(src_dir=dataset, target=DataPath(ds,\"gan_data\"), overwrite = True, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let us create training script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing train_gan.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_gan.py\n",
        "# KeraGAN trainer script\n",
        "\n",
        "import argparse\n",
        "import keragan\n",
        "import keras\n",
        "import os\n",
        "import glob\n",
        "from azureml.core.run import Run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"KeraGAN Trainer, version {}\".format(keragan.__version__))\n",
        "\n",
        "run = Run.get_context()\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"KeraGAN Trainer\")\n",
        "\n",
        "parser.add_argument(\"--path\",help=\"Azure ML Datastore and Dataset dir\")\n",
        "parser.add_argument(\"--size\",help=\"Image size to use\", default=512, type=int)\n",
        "parser.add_argument(\"--aspect_variance\",help=\"Allowed aspect variance\", default=0.5, type=float)\n",
        "parser.add_argument(\"--model_path\",help=\"Path to use for saving models\", default='models')\n",
        "parser.add_argument(\"--samples_path\",help=\"Path to use for saving samples\", default='samples')\n",
        "parser.add_argument(\"--save_npy_path\",help=\"Filename to save cached dataset for faster loading\")\n",
        "parser.add_argument(\"--limit\",help=\"Limit # of images to use\",type=int,default=None)\n",
        "parser.add_argument(\"--batch_size\",help=\"Minbatch size to use\",type=int,default=128)\n",
        "parser.add_argument(\"--save_interval\",help=\"Epochs between saving models\",type=int,default=100)\n",
        "parser.add_argument(\"--save_img_interval\",help=\"Epochs between generating image samples\",type=int,default=100)\n",
        "parser.add_argument(\"--print_interval\",help=\"Epochs between printing\",type=int,default=10)\n",
        "parser.add_argument(\"--sample_images\",help=\"View image sample\",action='store_const',default=False,const=True)\n",
        "parser.add_argument(\"--no_samples\",help=\"Number of sample images to generate during training\",type=int,default=10)\n",
        "parser.add_argument(\"--latent_dim\",help=\"Dimension of latent space\",type=int,default=256)\n",
        "parser.add_argument(\"--ignore_smaller\",help=\"Ignore images smaller than required size\",action='store_const',default=False,const=True)\n",
        "parser.add_argument(\"--crop\",help=\"Crop images to desired aspect ratio\",action='store_const',default=False,const=True)\n",
        "parser.add_argument(\"--epochs\",help=\"Number of epochs to train\",type=int,default=100)\n",
        "parser.add_argument(\"--lr\",help=\"Learning rate\",type=float,default=0.0001)\n",
        "args = parser.parse_args()\n",
        "\n",
        "args.height = args.size\n",
        "args.width = args.size\n",
        "args.optimizer = None\n",
        "\n",
        "dcgan_args = {\n",
        "    'width': args.width,\n",
        "    'height': args.height,\n",
        "    'model_path': args.model_path,\n",
        "    'samples_path': args.samples_path,\n",
        "    'optimizer': args.optimizer,\n",
        "    'lr': args.lr,\n",
        "    'latent_dim': args.latent_dim\n",
        "}\n",
        "gan = keragan.DCGAN(**dcgan_args)\n",
        "\n",
        "image_dataset_args = {\n",
        "    'path': args.path,\n",
        "    'height': args.height,\n",
        "    'width': args.width,\n",
        "    'aspect_variance': args.aspect_variance,\n",
        "    'save_npy_path': args.save_npy_path,\n",
        "    'ignore_smaller': args.ignore_smaller,\n",
        "    'limit': args.limit,\n",
        "    'crop': args.crop\n",
        "}\n",
        "imsrc = keragan.ImageDataset(**image_dataset_args)\n",
        "imsrc.load()\n",
        "print(imsrc.data,imsrc.data.shape[0])\n",
        "train = keragan.GANTrainer(image_dataset=imsrc,gan=gan,args=args)\n",
        "\n",
        "def callbk(tr):\n",
        "    if tr.gan.epoch % 20 == 0:\n",
        "        res = tr.gan.sample_images(n=3)\n",
        "        fig,ax = plt.subplots(1,len(res))\n",
        "        for i,v in enumerate(res):\n",
        "            ax[i].imshow(v[0])\n",
        "        run.log_image(\"Sample\",plot=plt)\n",
        "\n",
        "train.train(callbk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1694273752810
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment, Dataset, Datastore\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# Define the environment\n",
        "environment = Environment(name='keragan-env')\n",
        "environment.python.conda_dependencies = CondaDependencies.create(\n",
        "    conda_packages=['keras','tensorflow','tqdm','matplotlib'],\n",
        "    pip_packages=['azureml-core','azureml-defaults','imutils', 'opencv-python-headless','git+https://github.com/Marcusng01/keragan@506773f62c36c08a6efb7616181bc010526abf43']\n",
        ")\n",
        "\n",
        "gan_data = Dataset.File.from_files(path=(ds, 'gan_data'))\n",
        "script_params = [\n",
        "    '--path', gan_data.as_mount(),\n",
        "    '--model_path', './outputs/models',\n",
        "    '--samples_path', './outputs/samples',\n",
        "    '--batch_size', 32,\n",
        "    '--size', 512,\n",
        "    '--limit', 3000,\n",
        "    '--epochs', 10000\n",
        "]\n",
        "\n",
        "# Create a ScriptRunConfig\n",
        "script_run_config = ScriptRunConfig(\n",
        "    source_directory='.',\n",
        "    script='train_gan.py',\n",
        "    arguments=script_params,\n",
        "    compute_target=cluster,\n",
        "    environment=environment,\n",
        ")\n",
        "\n",
        "# Create an experiment\n",
        "experiment_name = 'KeraGAN'\n",
        "exp = Experiment(workspace=ws, name=experiment_name)\n",
        "\n",
        "# Submit the run\n",
        "run = exp.submit(config=script_run_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading resulting images\n",
        "\n",
        "After the experiment has completed, you can download resulting images to your local machine. If the experiment was long-running and the notebook session is lost, you can re-create it knowing the run id (which you can get from the portal). Otherwise you can use the same `run` variable from above (skip the cell below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1694325255733
        }
      },
      "outputs": [
        {
          "ename": "ServiceException",
          "evalue": "ServiceException:\n\tCode: 404\n\tMessage: (UserError) Run KeraGAN_1584048041337 was not found\n\tDetails:\n\n\tHeaders: {\n\t    \"Date\": \"Sun, 10 Sep 2023 17:38:11 GMT\",\n\t    \"Content-Type\": \"application/json; charset=utf-8\",\n\t    \"Transfer-Encoding\": \"chunked\",\n\t    \"Connection\": \"keep-alive\",\n\t    \"Vary\": \"Accept-Encoding\",\n\t    \"Request-Context\": \"appId=cid-v1:67969c6a-972f-47a9-8267-e09d830cc328\",\n\t    \"x-ms-response-type\": \"error\",\n\t    \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\",\n\t    \"X-Content-Type-Options\": \"nosniff\",\n\t    \"x-aml-cluster\": \"vienna-southeastasia-02\",\n\t    \"x-request-time\": \"0.028\",\n\t    \"Content-Encoding\": \"gzip\"\n\t}\n\tInnerException: {\n    \"additional_properties\": {\n        \"statusCode\": 404\n    },\n    \"error\": {\n        \"additional_properties\": {\n            \"debugInfo\": null\n        },\n        \"code\": \"UserError\",\n        \"severity\": null,\n        \"message\": \"Run KeraGAN_1584048041337 was not found\",\n        \"message_format\": \"Run {runId} was not found\",\n        \"message_parameters\": {\n            \"runId\": \"KeraGAN_1584048041337\"\n        },\n        \"reference_code\": null,\n        \"details_uri\": null,\n        \"target\": null,\n        \"details\": [],\n        \"inner_error\": {\n            \"additional_properties\": {},\n            \"code\": \"NotFoundError\",\n            \"inner_error\": null\n        },\n        \"additional_info\": null\n    },\n    \"correlation\": {\n        \"operation\": \"2c748f8cdfc91dda1f2dc20edb6cca42\",\n        \"request\": \"f4aefb7647626fcd\"\n    },\n    \"environment\": \"southeastasia\",\n    \"location\": \"southeastasia\",\n    \"time\": {},\n    \"component_name\": \"run-history\"\n}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mErrorResponseException\u001b[0m                    Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:590\u001b[0m, in \u001b[0;36mClientBase._execute_with_arguments\u001b[1;34m(self, func, args_list, *args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 590\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_api(func, \u001b[39m*\u001b[39;49margs_list, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mexcept\u001b[39;00m ErrorResponseException \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:245\u001b[0m, in \u001b[0;36mClientBase._call_api\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_with_base_arguments(func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:334\u001b[0m, in \u001b[0;36mClientBase._execute_with_base_arguments\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m total_retry \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretries \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretries\n\u001b[1;32m--> 334\u001b[0m \u001b[39mreturn\u001b[39;00m ClientBase\u001b[39m.\u001b[39;49m_execute_func_internal(\n\u001b[0;32m    335\u001b[0m     back_off, total_retry, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logger, func, _noop_reset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:368\u001b[0m, in \u001b[0;36mClientBase._execute_func_internal\u001b[1;34m(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m--> 368\u001b[0m     left_retry \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_retry(back_off, left_retry, total_retry, error, logger, func)\n\u001b[0;32m    370\u001b[0m reset_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:427\u001b[0m, in \u001b[0;36mClientBase._handle_retry\u001b[1;34m(cls, back_off, left_retry, total_retry, error, logger, func)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[39melif\u001b[39;00m error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m500\u001b[39m \u001b[39mand\u001b[39;00m error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m408\u001b[39m:\n\u001b[1;32m--> 427\u001b[0m         \u001b[39mraise\u001b[39;00m error\n\u001b[0;32m    428\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(error, ClientRequestError):\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:359\u001b[0m, in \u001b[0;36mClientBase._execute_func_internal\u001b[1;34m(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mClientBase: Calling \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with url \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(func_name, func_url))\n\u001b[1;32m--> 359\u001b[0m response \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(response, Response) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_retryable_status_code(response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m    361\u001b[0m         \u001b[39mand\u001b[39;00m left_retry \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# This is the handle the error case 1. response.raise_for_status only throws HTTPError exception.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39m# if the status_code is retryable and it is not the last retry, then the exception is thrown.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[39m# Otherwise, we will return the response directly.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\operations\\run_operations.py:506\u001b[0m, in \u001b[0;36mRunOperations.get_by_exp_id\u001b[1;34m(self, subscription_id, resource_group_name, workspace_name, experiment_id, run_id, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mErrorResponseException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[0;32m    508\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mErrorResponseException\u001b[0m: (UserError) Run KeraGAN_1584048041337 was not found",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mServiceException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32mc:\\Marcus\\Important Docs\\MLSA\\code\\submit_gan-copy.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Marcus/Important%20Docs/MLSA/code/submit_gan-copy.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Possible to skip this cell\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Marcus/Important%20Docs/MLSA/code/submit_gan-copy.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Run\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Marcus/Important%20Docs/MLSA/code/submit_gan-copy.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m run \u001b[39m=\u001b[39m Run(experiment\u001b[39m=\u001b[39;49mexp,run_id\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mKeraGAN_1584048041337\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# <-- provide run id from Azure Portal here\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Marcus/Important%20Docs/MLSA/code/submit_gan-copy.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m run\u001b[39m.\u001b[39mget_file_names()[:\u001b[39m10\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\core\\run.py:173\u001b[0m, in \u001b[0;36mRun.__init__\u001b[1;34m(self, experiment, run_id, outputs, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, experiment, run_id, outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize the Run object.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[39m    :param experiment: The containing experiment.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \n\u001b[0;32m    172\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     \u001b[39msuper\u001b[39;49m(Run, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(experiment, run_id, outputs\u001b[39m=\u001b[39;49moutputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_run \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_run_impl\\run_base.py:89\u001b[0m, in \u001b[0;36m_RunBase.__init__\u001b[1;34m(self, experiment, run_id, outputs, logs, _run_dto, _worker_pool, _user_agent, _ident, _batch_upload_metrics, py_wd, deny_list, flush_eager, redirect_output_stream, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m     87\u001b[0m py_wd \u001b[39m=\u001b[39m get_py_wd() \u001b[39mif\u001b[39;00m py_wd \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m py_wd\n\u001b[1;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client \u001b[39m=\u001b[39m RunHistoryFacade(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experiment, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_id, RUN_ORIGIN, run_dto\u001b[39m=\u001b[39;49m_run_dto,\n\u001b[0;32m     90\u001b[0m                                 worker_pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experiment\u001b[39m.\u001b[39;49mworkspace\u001b[39m.\u001b[39;49mservice_context\u001b[39m.\u001b[39;49mworker_pool,\n\u001b[0;32m     91\u001b[0m                                 outputs\u001b[39m=\u001b[39;49moutputs, py_wd\u001b[39m=\u001b[39;49mpy_wd, deny_list\u001b[39m=\u001b[39;49mdeny_list,\n\u001b[0;32m     92\u001b[0m                                 user_agent\u001b[39m=\u001b[39;49muser_agent, _parent_logger\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logger,\n\u001b[0;32m     93\u001b[0m                                 _batch_upload_metrics\u001b[39m=\u001b[39;49m_batch_upload_metrics, flush_eager\u001b[39m=\u001b[39;49mflush_eager)\n\u001b[0;32m     95\u001b[0m \u001b[39m# self._run_dto property does some time-expensive serialization\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m# so just materialize it once for use to populate all other fields\u001b[39;00m\n\u001b[0;32m     97\u001b[0m _run_dto_as_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_dto\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_run_impl\\run_history_facade.py:96\u001b[0m, in \u001b[0;36mRunHistoryFacade.__init__\u001b[1;34m(self, experiment, run_id, origin, run_dto, user_agent, worker_pool, outputs, py_wd, deny_list, _batch_upload_metrics, flush_eager, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msnapshots \u001b[39m=\u001b[39m SnapshotsClient(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment\u001b[39m.\u001b[39mworkspace\u001b[39m.\u001b[39mservice_context, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbase_kwargs)\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m MetricsClient(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment\u001b[39m.\u001b[39mworkspace\u001b[39m.\u001b[39mservice_context, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_id,\n\u001b[0;32m     95\u001b[0m                              use_batch\u001b[39m=\u001b[39m_batch_upload_metrics, flush_eager\u001b[39m=\u001b[39mflush_eager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbase_kwargs)\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_dto \u001b[39m=\u001b[39m run_dto \u001b[39mif\u001b[39;00m run_dto \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mget_run()\n\u001b[0;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_file_tracker \u001b[39m=\u001b[39m TrackFolders(py_wd, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39martifacts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_container_id, outputs, deny_list)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\run_client.py:78\u001b[0m, in \u001b[0;36mRunClient.get_run\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_run\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     73\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    Get detail of a run by its run_id\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m    This function could also be called from the super class,\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39m    ExperimentClient, for a specific run_id\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(RunClient, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mget_run(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_id, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\experiment_client.py:126\u001b[0m, in \u001b[0;36mExperimentClient.get_run\u001b[1;34m(self, run_id, caller, custom_headers, is_async)\u001b[0m\n\u001b[0;32m    122\u001b[0m kwargs \u001b[39m=\u001b[39m _generate_client_kwargs(\n\u001b[0;32m    123\u001b[0m     is_async\u001b[39m=\u001b[39mis_async, caller\u001b[39m=\u001b[39mcaller, custom_headers\u001b[39m=\u001b[39mcustom_headers)\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_id:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_with_experimentid_arguments(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mget_by_exp_id,\n\u001b[0;32m    127\u001b[0m                                                      run_id\u001b[39m=\u001b[39;49mrun_id,\n\u001b[0;32m    128\u001b[0m                                                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    129\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_with_experiment_arguments(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mget,\n\u001b[0;32m    130\u001b[0m                                                run_id\u001b[39m=\u001b[39mrun_id,\n\u001b[0;32m    131\u001b[0m                                                \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\experiment_client.py:265\u001b[0m, in \u001b[0;36mExperimentClient._execute_with_experimentid_arguments\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexperiment_id should not be None.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_with_arguments(func,\n\u001b[0;32m    266\u001b[0m                                     copy\u001b[39m.\u001b[39;49mdeepcopy(\n\u001b[0;32m    267\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experiment_arguments_with_experiment_id),\n\u001b[0;32m    268\u001b[0m                                     \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azureml\\_restclient\\clientbase.py:592\u001b[0m, in \u001b[0;36mClientBase._execute_with_arguments\u001b[1;34m(self, func, args_list, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_api(func, \u001b[39m*\u001b[39margs_list, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mexcept\u001b[39;00m ErrorResponseException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m ServiceException(e)\n",
            "\u001b[1;31mServiceException\u001b[0m: ServiceException:\n\tCode: 404\n\tMessage: (UserError) Run KeraGAN_1584048041337 was not found\n\tDetails:\n\n\tHeaders: {\n\t    \"Date\": \"Sun, 10 Sep 2023 17:38:11 GMT\",\n\t    \"Content-Type\": \"application/json; charset=utf-8\",\n\t    \"Transfer-Encoding\": \"chunked\",\n\t    \"Connection\": \"keep-alive\",\n\t    \"Vary\": \"Accept-Encoding\",\n\t    \"Request-Context\": \"appId=cid-v1:67969c6a-972f-47a9-8267-e09d830cc328\",\n\t    \"x-ms-response-type\": \"error\",\n\t    \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\",\n\t    \"X-Content-Type-Options\": \"nosniff\",\n\t    \"x-aml-cluster\": \"vienna-southeastasia-02\",\n\t    \"x-request-time\": \"0.028\",\n\t    \"Content-Encoding\": \"gzip\"\n\t}\n\tInnerException: {\n    \"additional_properties\": {\n        \"statusCode\": 404\n    },\n    \"error\": {\n        \"additional_properties\": {\n            \"debugInfo\": null\n        },\n        \"code\": \"UserError\",\n        \"severity\": null,\n        \"message\": \"Run KeraGAN_1584048041337 was not found\",\n        \"message_format\": \"Run {runId} was not found\",\n        \"message_parameters\": {\n            \"runId\": \"KeraGAN_1584048041337\"\n        },\n        \"reference_code\": null,\n        \"details_uri\": null,\n        \"target\": null,\n        \"details\": [],\n        \"inner_error\": {\n            \"additional_properties\": {},\n            \"code\": \"NotFoundError\",\n            \"inner_error\": null\n        },\n        \"additional_info\": null\n    },\n    \"correlation\": {\n        \"operation\": \"2c748f8cdfc91dda1f2dc20edb6cca42\",\n        \"request\": \"f4aefb7647626fcd\"\n    },\n    \"environment\": \"southeastasia\",\n    \"location\": \"southeastasia\",\n    \"time\": {},\n    \"component_name\": \"run-history\"\n}"
          ]
        }
      ],
      "source": [
        "#Possible to skip this cell\n",
        "from azureml.core import Run\n",
        "run = Run(experiment=exp,run_id='KeraGAN_1584048041337') # <-- provide run id from Azure Portal here\n",
        "run.get_file_names()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Следующая команда скачивает все сгенерированные сэмплы (которые наш скрипт помещал в директорию `outputs/samples`) на локальный компьютер. При этом в текущей директории появляется директория `outputs/samples` со всеми файлами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1694319492552
        }
      },
      "outputs": [],
      "source": [
        "run.download_files(prefix='outputs/samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting the Model and Inferring New Images\n",
        "\n",
        "Once we get the generator model, we can easily infer more images. To do that, let's find out the name of the latest model (it would have the highest epoch number) and download it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1694325241958
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'run' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/models/gen_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[43mrun\u001b[49m\u001b[38;5;241m.\u001b[39mget_file_names()))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(fnames[:\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m      3\u001b[0m no \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;241m19\u001b[39m:x\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)]), fnames))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
          ]
        }
      ],
      "source": [
        "fnames = list(filter(lambda x : x.startswith('outputs/models/gen_'), run.get_file_names()))\n",
        "print(fnames[:5])\n",
        "no = max(map(lambda x: int(x[19:x.find('.')]), fnames))\n",
        "fname = 'outputs/models/gen_{}.h5'.format(no)\n",
        "fname_wout_path = fname[fname.rfind('/')+1:]\n",
        "run.download_file(fname)\n",
        "print(fname_wout_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's load the model in Keras, and also find out the size of latent noise vector (it is equivalend to the input size of the network): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1694325235681
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-10 05:53:40.581024: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-10 05:53:50.015057: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-09-10 05:53:50.015224: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-09-10 05:53:50.015239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'fname_wout_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[43mfname_wout_path\u001b[49m, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m latent_dim\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fname_wout_path' is not defined"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "model = keras.models.load_model(fname_wout_path, compile=False)\n",
        "latent_dim=model.layers[0].input.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's generate random noise vector and call the model to generate 10 random images. Output of the network is in the range $[-1,1]$, so we need to scale it linearly to the range $[0,1]$ in order to be correctly displayed by `matplotlib`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694325235752
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "vec = np.random.normal(0,1,(10,latent_dim))\n",
        "res = model.predict(vec)\n",
        "res = (res+1.0)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1694325236866
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'res' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     ax[i]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mres\u001b[49m[i])\n\u001b[1;32m      5\u001b[0m     ax[i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig,ax = plt.subplots(1,10,figsize=(15,10))\n",
        "for i in range(10):\n",
        "    ax[i].imshow(res[i])\n",
        "    ax[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
